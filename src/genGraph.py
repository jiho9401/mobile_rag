import argparse
import json
import os
import time
import requests
from datetime import datetime
import numpy as np
import onnxruntime as ort
from transformers import AutoTokenizer
import random
from tqdm.auto import tqdm

class GraphGenerator:
    """LLMì„ í™œìš©í•œ ì§€ì‹ ê·¸ë˜í”„ ìƒì„±ê¸°"""
    
    def __init__(self, server_url, llm_model="gemma3:27b", embedding_model=None):
        """ì´ˆê¸°í™”: LLM ì„œë²„ ì„¤ì • ë° ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
        self.server_url = server_url
        self.llm_model = llm_model
        self.embedding_model = embedding_model
        
        print(f"ğŸ“Š ê·¸ë˜í”„ ìƒì„±ê¸° ì´ˆê¸°í™”...")
        print(f"   LLM ì„œë²„: {server_url}")
        print(f"   LLM ëª¨ë¸: {llm_model}")
        
        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ìˆëŠ” ê²½ìš°)
        self.ort_session = None
        self.tokenizer = None
        
        if embedding_model:
            try:
                print(f"ğŸ§  ONNX ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘: {embedding_model}")
                self.ort_session = ort.InferenceSession(embedding_model)
                self.tokenizer = AutoTokenizer.from_pretrained("intfloat/multilingual-e5-small")
                print("   ONNX ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ ONNX ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
                print("   ì„ë² ë”© ì—†ì´ ì§„í–‰í•©ë‹ˆë‹¤.")
    
    def compute_embedding(self, text):
        """í…ìŠ¤íŠ¸ ì„ë² ë”© ê³„ì‚° (E5 ëª¨ë¸ í˜•ì‹ ì§€ì›)"""
        if self.ort_session is None or self.tokenizer is None:
            return None
        
        # ë¹ˆ í…ìŠ¤íŠ¸ ì²´í¬
        if not text or text.strip() == "":
            return None
        
        try:
            # E5 ëª¨ë¸ í˜•ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ í¬ë§·íŒ…
            formatted_text = f"passage: {text}"
            
            # í† í°í™”
            inputs = self.tokenizer(
                formatted_text,
                padding=True,
                truncation=True,
                max_length=512,
                return_tensors="np"
            )
            
            # ëª¨ë¸ ì…ë ¥ ì¤€ë¹„
            model_inputs = {
                "input_ids": inputs["input_ids"]
            }
            
            # ì–´í…ì…˜ ë§ˆìŠ¤í¬ê°€ í•„ìš”í•˜ë©´ ì¶”ê°€
            if "attention_mask" in [inp.name for inp in self.ort_session.get_inputs()]:
                model_inputs["attention_mask"] = inputs["attention_mask"]
            
            # í† í° íƒ€ì… IDê°€ í•„ìš”í•˜ë©´ ì¶”ê°€
            if "token_type_ids" in [inp.name for inp in self.ort_session.get_inputs()]:
                if "token_type_ids" in inputs:
                    model_inputs["token_type_ids"] = inputs["token_type_ids"]
                else:
                    model_inputs["token_type_ids"] = np.zeros_like(inputs["input_ids"])
            
            # ONNX ëª¨ë¸ë¡œ ì„ë² ë”© ê³„ì‚°
            outputs = self.ort_session.run(None, model_inputs)
            
            # ì¶œë ¥ í˜•íƒœì— ë”°ë¼ ì„ë² ë”© ì¶”ì¶œ
            if len(outputs[0].shape) == 3:  # [batch, seq_len, hidden]
                embedding = outputs[0][:, 0, :]  # CLS í† í° ì„ë² ë”©
            else:  # [batch, hidden]
                embedding = outputs[0]  # ì´ë¯¸ í’€ë§ëœ ì„ë² ë”©
            
            # L2 ì •ê·œí™”
            norm = np.linalg.norm(embedding, axis=1, keepdims=True)
            normalized_embedding = embedding / norm
            
            return normalized_embedding[0].tolist()  # ë°°ì¹˜ ì°¨ì› ì œê±° ë° ë¦¬ìŠ¤íŠ¸ ë³€í™˜
        except Exception as e:
            print(f"âš ï¸ ì„ë² ë”© ê³„ì‚° ì˜¤ë¥˜: {e}")
            return None
    
    def call_llm(self, prompt):
        """LLM í˜¸ì¶œí•˜ì—¬ ì‘ë‹µ ìƒì„±"""
        url = f"{self.server_url}/api/generate"
        
        payload = {
            "model": self.llm_model,
            "prompt": prompt,
            "stream": False,
            "temperature": 0.7,  # ì°½ì˜ì„±ì„ ìœ„í•´ ì˜¨ë„ ë†’ì„
            "max_tokens": 2048
        }
        
        try:
            # íƒ€ì„ì•„ì›ƒ ì„¤ì • ì¶”ê°€ (180ì´ˆ)
            response = requests.post(url, json=payload, timeout=180)
            if response.status_code == 200:
                result = response.json()
                return result.get("response", "API ì‘ë‹µ ì˜¤ë¥˜: ì‘ë‹µ í•„ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                return f"API í˜¸ì¶œ ì˜¤ë¥˜: ìƒíƒœ ì½”ë“œ {response.status_code}"
        except requests.exceptions.Timeout:
            return "API í˜¸ì¶œ íƒ€ì„ì•„ì›ƒ: ì„œë²„ ì‘ë‹µì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤."
        except Exception as e:
            return f"API í˜¸ì¶œ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}"
    
    def generate_domain_knowledge(self, domain, num_nodes=500):
        """íŠ¹ì • ë„ë©”ì¸ì— ëŒ€í•œ ì§€ì‹ ì—”í‹°í‹° ìƒì„±"""
        prompt = f"""ë‹¤ìŒ ë„ë©”ì¸ì— ëŒ€í•œ í•µì‹¬ ê°œë…, ìš©ì–´, ì¸ë¬¼, ê¸°ê´€ ë“±ì„ JSON í˜•ì‹ìœ¼ë¡œ ìƒì„±í•´ì£¼ì„¸ìš”: {domain}

ê° ì—”í‹°í‹°ëŠ” ë‹¤ìŒ êµ¬ì¡°ë¥¼ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤:

{{
"id": "ê³ ìœ  ì‹ë³„ì",
"name": "ì—”í‹°í‹° ì´ë¦„",
"type": "ê°œë…/ì¸ë¬¼/ê¸°ê´€/ê¸°ìˆ  ë“± íƒ€ì…",
"description": "ìƒì„¸ ì„¤ëª…",
"properties": {{
"ì†ì„±1": "ê°’1",
"ì†ì„±2": "ê°’2"
}}
}}


ê°€ëŠ¥í•œ ë§ì€ ë‹¤ì–‘í•œ ì—”í‹°í‹°ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”. ì—”í‹°í‹°ë“¤ì€ í•´ë‹¹ ë„ë©”ì¸ì˜ í•µì‹¬ì ì¸ ìš”ì†Œë“¤ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
ê²°ê³¼ëŠ” JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”.
"""
        
        print(f"ğŸ§  '{domain}' ë„ë©”ì¸ì— ëŒ€í•œ ì§€ì‹ ì—”í‹°í‹° ìƒì„± ì¤‘...")
        
        # ì§„í–‰ ìƒí™© í‘œì‹œ
        with tqdm(total=1, desc="LLM ì—”í‹°í‹° ìƒì„±", unit="ìš”ì²­") as pbar:
            response = self.call_llm(prompt)
            pbar.update(1)
        
        try:
            # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ì—ì„œ)
            json_text = response
            if "```json" in response:
                json_text = response.split("```json")[1].split("```")[0].strip()
            elif "```" in response:
                json_text = response.split("```")[1].split("```")[0].strip()
            
            # JSON íŒŒì‹±
            entities = json.loads(json_text)
            print(f"âœ… {len(entities)}ê°œ ì—”í‹°í‹° ìƒì„± ì™„ë£Œ")
            return entities
        except Exception as e:
            print(f"âš ï¸ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
            print("ì›ë³¸ ì‘ë‹µ:")
            print(response)
            return []
    
    def generate_relationships(self, entities):
        """ì—”í‹°í‹° ê°„ ê´€ê³„ ìƒì„±"""
        # ì—”í‹°í‹° ëª©ë¡ ì¤€ë¹„
        entity_list = []
        for entity in entities:
            entity_info = f"ID: {entity['id']}, ì´ë¦„: {entity['name']}, ìœ í˜•: {entity['type']}"
            entity_list.append(entity_info)
        
        entity_text = "\n".join(entity_list)
        
        prompt = f"""ë‹¤ìŒì€ ì§€ì‹ ê·¸ë˜í”„ì— í¬í•¨ëœ ì—”í‹°í‹° ëª©ë¡ì…ë‹ˆë‹¤:

{entity_text}

ìœ„ ì—”í‹°í‹°ë“¤ ê°„ì˜ ì˜ë¯¸ ìˆëŠ” ê´€ê³„ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ìƒì„±í•´ì£¼ì„¸ìš”. ê´€ê³„ëŠ” ë‹¤ìŒ êµ¬ì¡°ë¥¼ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤:

{{
"source": "ì‹œì‘ ì—”í‹°í‹° ID",
"target": "ëŒ€ìƒ ì—”í‹°í‹° ID",
"relation": "ê´€ê³„ ìœ í˜• (ì˜ˆ: 'í¬í•¨í•œë‹¤', 'ì˜í–¥ì„ ë¯¸ì¹œë‹¤', 'ê°œë°œí–ˆë‹¤' ë“±)",
"weight": 0.1ê³¼ 1.0 ì‚¬ì´ì˜ ê´€ê³„ ê°•ë„,
"properties": {{
"ì†ì„±1": "ê°’1",
"ì†ì„±2": "ê°’2"
}}
}}


ìµœì†Œ {len(entities) * 2}ê°œì˜ ë‹¤ì–‘í•œ ê´€ê³„ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”. ì—”í‹°í‹°ë“¤ì˜ íŠ¹ì„±ì„ ê³ ë ¤í•˜ì—¬ ì˜ë¯¸ ìˆëŠ” ê´€ê³„ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
ê²°ê³¼ëŠ” JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”.
"""
        
        print(f"ğŸ”„ ì—”í‹°í‹° ê°„ ê´€ê³„ ìƒì„± ì¤‘...")
        
        # ì§„í–‰ ìƒí™© í‘œì‹œ
        with tqdm(total=1, desc="LLM ê´€ê³„ ìƒì„±", unit="ìš”ì²­") as pbar:
            response = self.call_llm(prompt)
            pbar.update(1)
        
        try:
            # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ì—ì„œ)
            json_text = response
            if "```json" in response:
                json_text = response.split("```json")[1].split("```")[0].strip()
            elif "```" in response:
                json_text = response.split("```")[1].split("```")[0].strip()
            
            # JSON íŒŒì‹±
            relationships = json.loads(json_text)
            print(f"âœ… {len(relationships)}ê°œ ê´€ê³„ ìƒì„± ì™„ë£Œ")
            return relationships
        except Exception as e:
            print(f"âš ï¸ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
            print("ì›ë³¸ ì‘ë‹µ:")
            print(response)
            return []
    
    def validate_knowledge_graph(self, nodes, edges):
        """ì§€ì‹ ê·¸ë˜í”„ì˜ ìœ íš¨ì„± ê²€ì¦"""
        print("ğŸ” ì§€ì‹ ê·¸ë˜í”„ ìœ íš¨ì„± ê²€ì¦ ì¤‘...")
        
        # ë…¸ë“œ ID ëª©ë¡ ìƒì„±
        node_ids = set(node["id"] for node in nodes)
        
        # ì—£ì§€ ê²€ì¦
        valid_edges = []
        invalid_edges = []
        
        for edge in tqdm(edges, desc="ê´€ê³„ ê²€ì¦", unit="ê´€ê³„"):
            source = edge.get("source")
            target = edge.get("target")
            
            # ì†ŒìŠ¤ì™€ íƒ€ê²Ÿì´ ë…¸ë“œ ëª©ë¡ì— ìˆëŠ”ì§€ í™•ì¸
            if source in node_ids and target in node_ids:
                valid_edges.append(edge)
            else:
                invalid_edges.append(edge)
        
        if invalid_edges:
            print(f"âš ï¸ {len(invalid_edges)}ê°œì˜ ìœ íš¨í•˜ì§€ ì•Šì€ ê´€ê³„ê°€ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        print(f"âœ… ê²€ì¦ ì™„ë£Œ: {len(valid_edges)}ê°œì˜ ìœ íš¨í•œ ê´€ê³„")
        return valid_edges
    
    def add_embeddings_to_nodes(self, nodes):
        """ë…¸ë“œì— ì„ë² ë”© ì¶”ê°€"""
        if self.ort_session is None:
            print("âš ï¸ ì„ë² ë”© ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•„ ì„ë² ë”©ì„ ì¶”ê°€í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return nodes
        
        print("ğŸ§® ë…¸ë“œì— ì„ë² ë”© ì¶”ê°€ ì¤‘...")
        
        for node in tqdm(nodes, desc="ì„ë² ë”© ì¶”ê°€", unit="ë…¸ë“œ"):
            # ì„ë² ë”© ê³„ì‚°ì„ ìœ„í•œ í…ìŠ¤íŠ¸ êµ¬ì„±
            text = f"{node['name']}. {node['description']}"
            
            # ì„ë² ë”© ê³„ì‚°
            embedding = self.compute_embedding(text)
            
            # ë…¸ë“œì— ì„ë² ë”© ì¶”ê°€
            if embedding is not None:
                node["embedding"] = embedding
        
        # ì„ë² ë”©ì´ ì¶”ê°€ëœ ë…¸ë“œ ìˆ˜ ê³„ì‚°
        nodes_with_embedding = sum(1 for node in nodes if "embedding" in node)
        print(f"âœ… {nodes_with_embedding}/{len(nodes)} ë…¸ë“œì— ì„ë² ë”© ì¶”ê°€ ì™„ë£Œ")
        
        return nodes
    
    def split_text_into_chunks(self, text, max_chunk_size=1500, overlap=100):
        """í…ìŠ¤íŠ¸ë¥¼ ì—¬ëŸ¬ ì²­í¬ë¡œ ë¶„í• """
        chunks = []
        
        # 1. ë¨¼ì € ë‹¤ì–‘í•œ êµ¬ë¶„ì„ ìœ¼ë¡œ ë¶„í•  ì‹œë„
        separators = [
            "*" * 80,  # 80ê°œì˜ ë³„í‘œ
            "=" * 80,  # 80ê°œì˜ ë“±í˜¸
            "-" * 80,  # 80ê°œì˜ í•˜ì´í”ˆ
        ]
        
        # êµ¬ë¶„ì„ ìœ¼ë¡œ ì´ˆê¸° ë¶„í• 
        initial_chunks = [text]
        for separator in separators:
            if separator in text:
                print(f"ğŸ” '{separator[:10]}...' êµ¬ë¶„ì„  ë°œê²¬")
                new_chunks = []
                for chunk in initial_chunks:
                    if separator in chunk:
                        parts = chunk.split(separator)
                        for i, part in enumerate(parts):
                            if i > 0:  # ì²« ë¶€ë¶„ì´ ì•„ë‹ˆë©´ êµ¬ë¶„ì„ ì„ í¬í•¨ì‹œì¼œ ë§¥ë½ ìœ ì§€
                                part = separator + part
                            if part.strip():
                                new_chunks.append(part.strip())
                    else:
                        if chunk.strip():
                            new_chunks.append(chunk)
                initial_chunks = new_chunks
        
        # ì´ˆê¸° êµ¬ë¶„ì„  ë¶„í•  ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
        if len(initial_chunks) > 1:
            print(f"âœ… êµ¬ë¶„ì„  ê¸°ì¤€ {len(initial_chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• ë¨")
            chunks = initial_chunks
        else:
            # êµ¬ë¶„ì„ ì´ ì—†ê±°ë‚˜ ë¶„í•  ê²°ê³¼ê°€ í•˜ë‚˜ë¿ì´ë©´ í…ìŠ¤íŠ¸ ìì²´ë¥¼ ì‚¬ìš©
            chunks = [text]
        
        # 2. í¬ê¸°ê°€ ë„ˆë¬´ í° ì²­í¬ëŠ” ì¶”ê°€ ë¶„í• 
        final_chunks = []
        for chunk in chunks:
            if len(chunk) <= max_chunk_size:
                final_chunks.append(chunk)
            else:
                # í¬ê¸° ê¸°ë°˜ ì¶”ê°€ ë¶„í• 
                sub_chunks = self._size_based_split(chunk, max_chunk_size, overlap)
                final_chunks.extend(sub_chunks)
        
        print(f"âœ… ìµœì¢…ì ìœ¼ë¡œ {len(final_chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• ë¨")
        return final_chunks
    
    def _size_based_split(self, text, max_chunk_size=1500, overlap=100):
        """í…ìŠ¤íŠ¸ë¥¼ í¬ê¸° ê¸°ë°˜ìœ¼ë¡œ ë¶„í• í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜"""
        chunks = []
        
        # ë¨¼ì € ë‹¨ë½ìœ¼ë¡œ ë¶„í•  ì‹œë„
        paragraphs = text.split("\n\n")
        if len(paragraphs) > 1:
            current_chunk = ""
            for para in paragraphs:
                para = para.strip()
                if not para:  # ë¹ˆ ë‹¨ë½ ê±´ë„ˆë›°ê¸°
                    continue
                
                # í˜„ì¬ ì²­í¬ê°€ ë¹„ì–´ìˆê±°ë‚˜, ë‹¨ë½ì„ ì¶”ê°€í•´ë„ ìµœëŒ€ í¬ê¸°ë¥¼ ì´ˆê³¼í•˜ì§€ ì•ŠëŠ” ê²½ìš°
                if not current_chunk or len(current_chunk) + len(para) + 2 <= max_chunk_size:
                    if current_chunk:
                        current_chunk += "\n\n" + para
                    else:
                        current_chunk = para
                else:
                    # í˜„ì¬ ì²­í¬ë¥¼ ì €ì¥í•˜ê³  ìƒˆ ì²­í¬ ì‹œì‘
                    chunks.append(current_chunk)
                    current_chunk = para
            
            # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
            if current_chunk:
                chunks.append(current_chunk)
            
            # ì²­í¬ê°€ ì˜ ë¶„í• ë˜ì—ˆìœ¼ë©´ ë°˜í™˜
            if len(chunks) > 1:
                return chunks
        
        # ë‹¨ë½ ë¶„í• ì´ íš¨ê³¼ì ì´ì§€ ì•Šìœ¼ë©´ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
        start = 0
        while start < len(text):
            end = min(start + max_chunk_size, len(text))
            
            # ë‹¤ìŒ ë¬¸ì¥ ê²½ê³„ê¹Œì§€ í™•ì¥ (ê°€ëŠ¥í•œ ê²½ìš°)
            if end < len(text):
                # ë‹¤ì–‘í•œ ë¬¸ì¥ ë ê¸°í˜¸ ì°¾ê¸°
                sentence_ends = []
                for punct in ['. ', '? ', '! ', '.\n', '?\n', '!\n']:
                    pos = text.rfind(punct, start, min(end + 200, len(text)))
                    if pos != -1:
                        sentence_ends.append(pos + len(punct) - 1)
                
                if sentence_ends:
                    end = max(sentence_ends) + 1
            
            # ì²­í¬ ì¶”ê°€
            chunk_text = text[start:end].strip()
            if chunk_text:  # ë¹ˆ ì²­í¬ëŠ” ì¶”ê°€í•˜ì§€ ì•ŠìŒ
                chunks.append(chunk_text)
            
            # ë¬´í•œ ë£¨í”„ ë°©ì§€
            if end <= start:
                start = start + max_chunk_size // 2  # ê°•ì œë¡œ ì´ë™
            else:
                start = end - overlap  # ì˜¤ë²„ë© ì ìš©
        
        return chunks
    
    def extract_entities_from_chunk(self, text_chunk, max_entities=100, existing_names=None):
        """í…ìŠ¤íŠ¸ ì²­í¬ì—ì„œ ì—”í‹°í‹° ì¶”ì¶œ"""
        if existing_names is None:
            existing_names = set()
        
        # ë„ˆë¬´ ì§§ì€ ì²­í¬ëŠ” ê±´ë„ˆë›°ê¸°
        if len(text_chunk.strip()) < 50:
            print("âš ï¸ ì²­í¬ê°€ ë„ˆë¬´ ì§§ì•„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return []
        
        existing_info = ""
        if existing_names:
            # ë„ˆë¬´ ë§ì€ ì´ë¦„ì„ í”„ë¡¬í”„íŠ¸ì— í¬í•¨í•˜ì§€ ì•Šë„ë¡ ì œí•œ
            sample_names = list(existing_names)[:20]
            existing_info = f"\n\nì£¼ì˜: ë‹¤ìŒ ì´ë¦„ê³¼ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ìƒˆë¡œìš´ ì—”í‹°í‹°ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”: {', '.join(sample_names)}"
        
        prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ ë¶„ì„í•˜ì—¬ í•µì‹¬ ê°œë…, ìš©ì–´, ì¸ë¬¼, ê¸°ê´€ ë“±ì„ JSON í˜•ì‹ìœ¼ë¡œ ìƒì„±í•´ì£¼ì„¸ìš”:

{text_chunk}

ê° ì—”í‹°í‹°ëŠ” ë‹¤ìŒ êµ¬ì¡°ë¥¼ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤:

{{
"id": "ì„ì‹œID",
"name": "ì—”í‹°í‹° ì´ë¦„",
"type": "ê°œë…/ì¸ë¬¼/ê¸°ê´€/ê¸°ìˆ  ë“± íƒ€ì…",
"description": "ìƒì„¸ ì„¤ëª…",
"properties": {{
"ì†ì„±1": "ê°’1",
"ì†ì„±2": "ê°’2"
}}
}}

ê°€ëŠ¥í•œ ë§ì€(ìµœì†Œ {max_entities}ê°œ) ë‹¤ì–‘í•œ ì—”í‹°í‹°ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”. í…ìŠ¤íŠ¸ì—ì„œ ëª…í™•í•˜ê²Œ ì–¸ê¸‰ëœ í•µì‹¬ì ì¸ ìš”ì†Œë“¤ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.{existing_info}
ê²°ê³¼ëŠ” JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”.
"""
        
        with tqdm(total=1, desc="LLM ì²­í¬ ì²˜ë¦¬", unit="ìš”ì²­") as pbar:
            response = self.call_llm(prompt)
            pbar.update(1)
        
        try:
            # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ì—ì„œ)
            json_text = response
            if "```json" in response:
                json_text = response.split("```json")[1].split("```")[0].strip()
            elif "```" in response:
                parts = response.split("```")
                if len(parts) >= 3:  # ìµœì†Œí•œ ì•, ì½”ë“œ, ë’¤ ì„¸ ë¶€ë¶„ì´ ìˆì–´ì•¼ í•¨
                    json_text = parts[1].strip()
                    # 'json' íƒœê·¸ê°€ ìˆì„ ê²½ìš° ì œê±°
                    if json_text.startswith("json"):
                        json_text = json_text[4:].strip() 
            
            # íŠ¹ìˆ˜ ë¬¸ìë‚˜ ê³µë°± ì •ë¦¬
            json_text = json_text.strip()
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                entities = json.loads(json_text)
                if not isinstance(entities, list):
                    print("âš ï¸ JSONì´ ë°°ì—´ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")
                    entities = []
                return entities
            except json.JSONDecodeError as e:
                print(f"âš ï¸ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
                print("JSON í…ìŠ¤íŠ¸ ì²˜ìŒ 200ì:")
                print(json_text[:200] + "..." if len(json_text) > 200 else json_text)
                # ë¹ˆ ë°°ì—´ ë°˜í™˜í•˜ì—¬ í”„ë¡œì„¸ìŠ¤ ê³„ì† ì§„í–‰
                return []
        except Exception as e:
            print(f"âš ï¸ ì²­í¬ ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            return []
    
    def generate_domain_knowledge_from_text(self, text_file, num_nodes=500):
        """í…ìŠ¤íŠ¸ íŒŒì¼ì„ ê¸°ë°˜ìœ¼ë¡œ ì§€ì‹ ì—”í‹°í‹° ìƒì„±"""
        try:
            # í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸°
            with open(text_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            print(f"ğŸ“„ í…ìŠ¤íŠ¸ íŒŒì¼ '{text_file}' ë¡œë“œ ì™„ë£Œ ({len(content)} ì)")
        except Exception as e:
            print(f"âŒ í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
            return []
        
        # í…ìŠ¤íŠ¸ ì²­í¬ë¡œ ë¶„í• 
        chunks = self.split_text_into_chunks(content, max_chunk_size=2000)
        print(f"ğŸ“‘ í…ìŠ¤íŠ¸ë¥¼ {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• ")
        
        all_entities = []
        existing_entity_names = set()  # ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•œ ì´ë¦„ ì¶”ì 
        
        # ê° ì²­í¬ë³„ë¡œ ì—”í‹°í‹° ìƒì„±
        for i, chunk in enumerate(chunks):
            print(f"ğŸ” ì²­í¬ {i+1}/{len(chunks)} ì²˜ë¦¬ ì¤‘...")
            
            # ê° ì²­í¬ ì²˜ë¦¬ì— ì˜ˆì™¸ í•¸ë“¤ë§ ì¶”ê°€
            try:
                chunk_entities = self.extract_entities_from_chunk(
                    chunk, 
                    100,  # ê° ì²­í¬ì—ì„œ ì¶”ì¶œí•  ìµœëŒ€ ì—”í‹°í‹° ìˆ˜ë¥¼ 100ìœ¼ë¡œ ì¦ê°€
                    existing_entity_names
                )
                
                # ì¤‘ë³µ ë°©ì§€í•˜ë©´ì„œ ì—”í‹°í‹° ì¶”ê°€
                for entity in chunk_entities:
                    if "name" in entity and entity["name"].lower() not in existing_entity_names:
                        all_entities.append(entity)
                        existing_entity_names.add(entity["name"].lower())
                
                print(f"âœ… í˜„ì¬ê¹Œì§€ {len(all_entities)} ì—”í‹°í‹° ìƒì„±")
                
                # ëª©í‘œ ë…¸ë“œ ìˆ˜ ë‹¬ì„± ì²´í¬ ì œê±° - ëª¨ë“  ì²­í¬ë¥¼ ì²˜ë¦¬í•˜ë„ë¡ í•¨
            except Exception as e:
                print(f"âš ï¸ ì²­í¬ {i+1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                continue  # ë‹¤ìŒ ì²­í¬ë¡œ ì§„í–‰
        
        # ì•„ì§ ëª©í‘œ ë…¸ë“œ ìˆ˜ì— ë„ë‹¬í•˜ì§€ ëª»í–ˆê³  ì¼ë¶€ ì—”í‹°í‹°ë¼ë„ ìƒì„±ë˜ì—ˆìœ¼ë©´ ì¶”ê°€ ìƒì„± ì‹œë„
        if len(all_entities) < num_nodes and len(all_entities) > 0:
            try:
                print(f"ğŸ”„ ì¶”ê°€ ì—”í‹°í‹° ìƒì„± ì¤‘... ({len(all_entities)}/{num_nodes})")
                additional_entities = self.generate_additional_entities(
                    content, all_entities, num_nodes - len(all_entities)
                )
                
                # ì¤‘ë³µ ë°©ì§€í•˜ë©´ì„œ ì¶”ê°€ ì—”í‹°í‹° ë³‘í•©
                for entity in additional_entities:
                    if "name" in entity and entity["name"].lower() not in existing_entity_names:
                        all_entities.append(entity)
                        existing_entity_names.add(entity["name"].lower())
            except Exception as e:
                print(f"âš ï¸ ì¶”ê°€ ì—”í‹°í‹° ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        if not all_entities:
            print("âš ï¸ ì—”í‹°í‹°ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì—”í‹°í‹°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
            # ê¸°ë³¸ ì—”í‹°í‹° ìƒì„±
            domain = os.path.basename(text_file)
            all_entities = self.generate_domain_knowledge(domain, min(20, num_nodes))
        
        # ID ì¬í• ë‹¹
        for i, entity in enumerate(all_entities):
            entity["id"] = f"E{i+1:03d}"
        
        print(f"âœ… ì´ {len(all_entities)}ê°œ ì—”í‹°í‹° ìƒì„± ì™„ë£Œ")
        return all_entities
    
    def generate_additional_entities(self, full_text, existing_entities, count=20):
        """ê¸°ì¡´ ì—”í‹°í‹°ë¥¼ ê³ ë ¤í•˜ì—¬ ì¶”ê°€ ì—”í‹°í‹° ìƒì„±"""
        # ê¸°ì¡´ ì—”í‹°í‹° ì •ë³´ ì¤€ë¹„
        entity_names = [entity["name"] for entity in existing_entities[:30]]  # ì²˜ìŒ 30ê°œë§Œ
        entity_info = ", ".join(entity_names)
        
        prompt = f"""ë‹¤ìŒì€ í…ìŠ¤íŠ¸ì—ì„œ ì´ë¯¸ ì¶”ì¶œí•œ ì£¼ìš” ì—”í‹°í‹°ë“¤ì…ë‹ˆë‹¤:
{entity_info}

ì´ì œ ë™ì¼í•œ í…ìŠ¤íŠ¸ì—ì„œ ìœ„ì— ë‚˜ì—´ë˜ì§€ ì•Šì€ ìƒˆë¡œìš´ ì—”í‹°í‹°ë¥¼ ì •í™•íˆ {count}ê°œ ìƒì„±í•´ì£¼ì„¸ìš”.
í…ìŠ¤íŠ¸ ë§¥ë½ì— ë§ëŠ” ê´€ë ¨ì„± ìˆëŠ” ê°œë…, ìš©ì–´, ì¸ë¬¼, ê¸°ê´€ ë“±ì„ ì‹ë³„í•´ì•¼ í•©ë‹ˆë‹¤.

ê° ì—”í‹°í‹°ëŠ” ë‹¤ìŒ êµ¬ì¡°ë¥¼ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤:

{{
"id": "ì„ì‹œID",
"name": "ì—”í‹°í‹° ì´ë¦„ (ìœ„ ëª©ë¡ì— ì—†ëŠ” ìƒˆë¡œìš´ ì´ë¦„)",
"type": "ê°œë…/ì¸ë¬¼/ê¸°ê´€/ê¸°ìˆ  ë“± íƒ€ì…",
"description": "ìƒì„¸ ì„¤ëª…",
"properties": {{
"ì†ì„±1": "ê°’1",
"ì†ì„±2": "ê°’2"
}}
}}

ì¤‘ìš”: ì •í™•íˆ {count}ê°œì˜ ìƒˆë¡œìš´ ì—”í‹°í‹°ë¥¼ JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”.
"""
        
        with tqdm(total=1, desc="LLM ì¶”ê°€ ì—”í‹°í‹° ìƒì„±", unit="ìš”ì²­") as pbar:
            response = self.call_llm(prompt)
            pbar.update(1)
        
        try:
            # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ì—ì„œ)
            json_text = response
            if "```json" in response:
                json_text = response.split("```json")[1].split("```")[0].strip()
            elif "```" in response:
                json_text = response.split("```")[1].split("```")[0].strip()
            
            # JSON íŒŒì‹±
            entities = json.loads(json_text)
            return entities
        except Exception as e:
            print(f"âš ï¸ ì¶”ê°€ ì—”í‹°í‹° ìƒì„± ì˜¤ë¥˜: {e}")
            return []
    
    def create_knowledge_graph(self, domain=None, text_file=None, num_nodes=500, output_file=None):
        """ì§€ì‹ ê·¸ë˜í”„ ìƒì„± ë° ì €ì¥ (ë„ë©”ì¸ëª… ë˜ëŠ” í…ìŠ¤íŠ¸ íŒŒì¼ ê¸°ë°˜)"""
        if text_file:
            print(f"\nğŸŒ '{text_file}' í…ìŠ¤íŠ¸ íŒŒì¼ ê¸°ë°˜ ì§€ì‹ ê·¸ë˜í”„ ìƒì„± ì‹œì‘")
            # í…ìŠ¤íŠ¸ íŒŒì¼ ê¸°ë°˜ ì—”í‹°í‹° ìƒì„±
            entities = self.generate_domain_knowledge_from_text(text_file, num_nodes)
            if domain is None:
                domain = os.path.basename(text_file)  # íŒŒì¼ëª…ì„ ë„ë©”ì¸ëª…ìœ¼ë¡œ ì‚¬ìš©
        else:
            print(f"\nğŸŒ '{domain}' ë„ë©”ì¸ì˜ ì§€ì‹ ê·¸ë˜í”„ ìƒì„± ì‹œì‘")
            # ë„ë©”ì¸ëª… ê¸°ë°˜ ì—”í‹°í‹° ìƒì„±
            entities = self.generate_domain_knowledge(domain, num_nodes)
        
        if not entities:
            print("âŒ ì—”í‹°í‹° ìƒì„± ì‹¤íŒ¨")
            return None
        
        # ê´€ê³„ ìƒì„±
        relationships = self.generate_relationships(entities)
        if not relationships:
            print("âŒ ê´€ê³„ ìƒì„± ì‹¤íŒ¨")
            return None
        
        # ê´€ê³„ ìœ íš¨ì„± ê²€ì¦
        valid_relationships = self.validate_knowledge_graph(entities, relationships)
        
        # ì„ë² ë”© ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
        entities_with_embeddings = self.add_embeddings_to_nodes(entities)
        
        # ì§€ì‹ ê·¸ë˜í”„ êµ¬ì„±
        knowledge_graph = {
            "metadata": {
                "domain": domain,
                "created_at": datetime.now().isoformat(),
                "node_count": len(entities_with_embeddings),
                "edge_count": len(valid_relationships)
            },
            "nodes": entities_with_embeddings,
            "edges": valid_relationships
        }
        
        # ì§€ì‹ ê·¸ë˜í”„ ì €ì¥
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(knowledge_graph, f, ensure_ascii=False, indent=2)
            print(f"âœ… ì§€ì‹ ê·¸ë˜í”„ê°€ {output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        print(f"âœ… ì§€ì‹ ê·¸ë˜í”„ ìƒì„± ì™„ë£Œ: ë…¸ë“œ {len(entities_with_embeddings)}ê°œ, ì—£ì§€ {len(valid_relationships)}ê°œ")
        return knowledge_graph
    
    def enrich_knowledge_graph(self, graph_file, output_file=None):
        """ê¸°ì¡´ ì§€ì‹ ê·¸ë˜í”„ë¥¼ í™•ì¥ ë° ë³´ê°•"""
        print(f"\nğŸ”„ ê¸°ì¡´ ì§€ì‹ ê·¸ë˜í”„ ë³´ê°• ì‹œì‘: {graph_file}")
        
        # ê¸°ì¡´ ê·¸ë˜í”„ ë¡œë“œ
        try:
            with open(graph_file, 'r', encoding='utf-8') as f:
                graph_data = json.load(f)
            
            nodes = graph_data.get("nodes", [])
            edges = graph_data.get("edges", [])
            metadata = graph_data.get("metadata", {})
            
            print(f"   ë¡œë“œëœ ê·¸ë˜í”„: ë…¸ë“œ {len(nodes)}ê°œ, ì—£ì§€ {len(edges)}ê°œ")
        except Exception as e:
            print(f"âŒ ê·¸ë˜í”„ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
        
        # ë…¸ë“œ ëª©ë¡ í…ìŠ¤íŠ¸ êµ¬ì„±
        node_list = []
        for node in nodes:
            node_info = f"ID: {node['id']}, ì´ë¦„: {node['name']}, ìœ í˜•: {node['type']}"
            node_list.append(node_info)
        
        node_text = "\n".join(node_list)
        
        # ìƒˆ ë…¸ë“œ ìƒì„± í”„ë¡¬í”„íŠ¸
        new_nodes_prompt = f"""ë‹¤ìŒì€ ê¸°ì¡´ ì§€ì‹ ê·¸ë˜í”„ì˜ ë…¸ë“œ ëª©ë¡ì…ë‹ˆë‹¤:

{node_text}

ìœ„ ì§€ì‹ ê·¸ë˜í”„ë¥¼ ë³´ê°•í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ë…¸ë“œ 5-10ê°œë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ìƒì„±í•´ì£¼ì„¸ìš”. ê¸°ì¡´ ë…¸ë“œì™€ ê´€ë ¨ë˜ì§€ë§Œ ëˆ„ë½ëœ ì¤‘ìš”í•œ ê°œë…ì´ë‚˜ ì—”í‹°í‹°ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.
ê° ë…¸ë“œëŠ” ë‹¤ìŒ êµ¬ì¡°ë¥¼ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤:

{{
"id": "ê³ ìœ  ì‹ë³„ì (ê¸°ì¡´ IDì™€ ì¤‘ë³µë˜ì§€ ì•Šê²Œ)",
"name": "ì—”í‹°í‹° ì´ë¦„",
"type": "ê°œë…/ì¸ë¬¼/ê¸°ê´€/ê¸°ìˆ  ë“± íƒ€ì…",
"description": "ìƒì„¸ ì„¤ëª…",
"properties": {{
"ì†ì„±1": "ê°’1",
"ì†ì„±2": "ê°’2"
}}
}}


ê²°ê³¼ëŠ” JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”.
"""
        
        # ìƒˆ ë…¸ë“œ ìƒì„±
        print("ğŸ§  ìƒˆ ë…¸ë“œ ìƒì„± ì¤‘...")
        response = self.call_llm(new_nodes_prompt)
        
        try:
            # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
            json_text = response
            if "```json" in response:
                json_text = response.split("```json")[1].split("```")[0].strip()
            elif "```" in response:
                json_text = response.split("```")[1].split("```")[0].strip()
            
            # JSON íŒŒì‹±
            new_nodes = json.loads(json_text)
            print(f"âœ… {len(new_nodes)}ê°œ ìƒˆ ë…¸ë“œ ìƒì„± ì™„ë£Œ")
            
            # ìƒˆ ë…¸ë“œì— ì„ë² ë”© ì¶”ê°€
            new_nodes_with_embeddings = self.add_embeddings_to_nodes(new_nodes)
            
            # ê¸°ì¡´ IDì™€ ì¤‘ë³µ ë°©ì§€
            existing_ids = set(node["id"] for node in nodes)
            valid_new_nodes = []
            
            for node in new_nodes_with_embeddings:
                if node["id"] in existing_ids:
                    # ID ì¶©ëŒ ì‹œ ìƒˆ ID ìƒì„±
                    node["id"] = f"n{len(existing_ids) + len(valid_new_nodes) + 1}"
                
                valid_new_nodes.append(node)
                existing_ids.add(node["id"])
            
            # ëª¨ë“  ë…¸ë“œ ëª©ë¡ ì—…ë°ì´íŠ¸
            all_nodes = nodes + valid_new_nodes
            
            # ìƒˆ ê´€ê³„ ìƒì„± ì¤€ë¹„
            all_node_list = []
            for node in all_nodes:
                node_info = f"ID: {node['id']}, ì´ë¦„: {node['name']}, ìœ í˜•: {node['type']}"
                all_node_list.append(node_info)
            
            all_node_text = "\n".join(all_node_list)
            
            # ìƒˆ ê´€ê³„ ìƒì„± í”„ë¡¬í”„íŠ¸
            new_edges_prompt = f"""ë‹¤ìŒì€ ì—…ë°ì´íŠ¸ëœ ì§€ì‹ ê·¸ë˜í”„ì˜ ë…¸ë“œ ëª©ë¡ì…ë‹ˆë‹¤:

{all_node_text}

ê¸°ì¡´ ë…¸ë“œì™€ ìƒˆ ë…¸ë“œ ê°„ì˜ ìƒˆë¡œìš´ ê´€ê³„ 10-15ê°œë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ìƒì„±í•´ì£¼ì„¸ìš”. íŠ¹íˆ ìƒˆë¡œ ì¶”ê°€ëœ ë…¸ë“œê°€ ê¸°ì¡´ ë…¸ë“œì™€ ì–´ë–»ê²Œ ì—°ê²°ë˜ëŠ”ì§€ í‘œí˜„í•´ì£¼ì„¸ìš”.
ê° ê´€ê³„ëŠ” ë‹¤ìŒ êµ¬ì¡°ë¥¼ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤:

{{
"source": "ì‹œì‘ ì—”í‹°í‹° ID",
"target": "ëŒ€ìƒ ì—”í‹°í‹° ID",
"relation": "ê´€ê³„ ìœ í˜•",
"weight": 0.1ê³¼ 1.0 ì‚¬ì´ì˜ ê´€ê³„ ê°•ë„,
"properties": {{
"ì†ì„±1": "ê°’1",
"ì†ì„±2": "ê°’2"
}}
}}


ê²°ê³¼ëŠ” JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”.
"""
            
            # ìƒˆ ê´€ê³„ ìƒì„±
            print("ğŸ”„ ìƒˆ ê´€ê³„ ìƒì„± ì¤‘...")
            response = self.call_llm(new_edges_prompt)
            
            try:
                # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
                json_text = response
                if "```json" in response:
                    json_text = response.split("```json")[1].split("```")[0].strip()
                elif "```" in response:
                    json_text = response.split("```")[1].split("```")[0].strip()
                
                # JSON íŒŒì‹±
                new_edges = json.loads(json_text)
                print(f"âœ… {len(new_edges)}ê°œ ìƒˆ ê´€ê³„ ìƒì„± ì™„ë£Œ")
                
                # ê´€ê³„ ìœ íš¨ì„± ê²€ì¦
                valid_new_edges = self.validate_knowledge_graph(all_nodes, new_edges)
                
                # ëª¨ë“  ê´€ê³„ ëª©ë¡ ì—…ë°ì´íŠ¸
                all_edges = edges + valid_new_edges
                
                # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
                updated_metadata = metadata.copy()
                updated_metadata["updated_at"] = datetime.now().isoformat()
                updated_metadata["node_count"] = len(all_nodes)
                updated_metadata["edge_count"] = len(all_edges)
                updated_metadata["enrichment_count"] = updated_metadata.get("enrichment_count", 0) + 1
                
                # ì—…ë°ì´íŠ¸ëœ ê·¸ë˜í”„ êµ¬ì„±
                updated_graph = {
                    "metadata": updated_metadata,
                    "nodes": all_nodes,
                    "edges": all_edges
                }
                
                # ì—…ë°ì´íŠ¸ëœ ê·¸ë˜í”„ ì €ì¥
                if not output_file:
                    output_file = graph_file
                
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(updated_graph, f, ensure_ascii=False, indent=2)
                
                print(f"âœ… ë³´ê°•ëœ ì§€ì‹ ê·¸ë˜í”„ê°€ {output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                print(f"   ì´ ë…¸ë“œ: {len(all_nodes)}ê°œ (ìƒˆ ë…¸ë“œ: {len(valid_new_nodes)}ê°œ)")
                print(f"   ì´ ê´€ê³„: {len(all_edges)}ê°œ (ìƒˆ ê´€ê³„: {len(valid_new_edges)}ê°œ)")
                
                return updated_graph
            except Exception as e:
                print(f"âŒ ìƒˆ ê´€ê³„ ìƒì„± ì‹¤íŒ¨: {e}")
                return None
        except Exception as e:
            print(f"âŒ ìƒˆ ë…¸ë“œ ìƒì„± ì‹¤íŒ¨: {e}")
            return None

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ëª…ë ¹ì¤„ ì¸í„°í˜ì´ìŠ¤ ì„¤ì •
    parser = argparse.ArgumentParser(description="LLM ê¸°ë°˜ ì§€ì‹ ê·¸ë˜í”„ ìƒì„±ê¸°")
    parser.add_argument("--server", "-s", help="LLM ì„œë²„ URL")
    parser.add_argument("--llm", "-l", help="LLM ëª¨ë¸ ì´ë¦„", default="gemma3:27b")
    parser.add_argument("--domain", "-d", help="ì§€ì‹ ê·¸ë˜í”„ ë„ë©”ì¸", default="ëŒ€í•™êµ ì•ˆë‚´")
    parser.add_argument("--nodes", "-n", type=int, help="ìƒì„±í•  ë…¸ë“œ ìˆ˜", default=1000)
    parser.add_argument("--output", "-o", help="ì¶œë ¥ JSON íŒŒì¼ ê²½ë¡œ", default="knowledge_graph.json")
    parser.add_argument("--embedding", "-e", help="ONNX ì„ë² ë”© ëª¨ë¸ íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--enrich", "-r", help="ë³´ê°•í•  ê¸°ì¡´ ê·¸ë˜í”„ íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--text", "-t", help="ì§€ì‹ ê·¸ë˜í”„ ìƒì„±ì— ì‚¬ìš©í•  í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ")
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸŒ LLM ê¸°ë°˜ ì§€ì‹ ê·¸ë˜í”„ ìƒì„±ê¸°")
    print("=" * 60)
    
    # ê·¸ë˜í”„ ìƒì„±ê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    generator = GraphGenerator(
        server_url=args.server,
        llm_model=args.llm,
        embedding_model=args.embedding
    )
    
    print(f"\nâœ… ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ!")
    
    # ê¸°ì¡´ ê·¸ë˜í”„ ë³´ê°• ëª¨ë“œ
    if args.enrich:
        generator.enrich_knowledge_graph(
            graph_file=args.enrich,
            output_file=args.output
        )
    # ìƒˆ ê·¸ë˜í”„ ìƒì„± ëª¨ë“œ (í…ìŠ¤íŠ¸ íŒŒì¼ ë˜ëŠ” ë„ë©”ì¸ ê¸°ë°˜)
    else:
        # í…ìŠ¤íŠ¸ íŒŒì¼ì´ ì§€ì •ë˜ì§€ ì•Šì•˜ê³  ê¸°ë³¸ í…ìŠ¤íŠ¸ íŒŒì¼ì´ ìˆëŠ” ê²½ìš° ìë™ìœ¼ë¡œ ì‚¬ìš©
        if not args.text and os.path.exists("context.txt"):
            args.text = "context.txt"
            print(f"ğŸ“„ ê¸°ë³¸ í…ìŠ¤íŠ¸ íŒŒì¼ 'context.txt'ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        
        generator.create_knowledge_graph(
            domain=args.domain,
            text_file=args.text,
            num_nodes=args.nodes,
            output_file=args.output
        )

if __name__ == "__main__":
    main()