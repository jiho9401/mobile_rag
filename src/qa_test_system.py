import json
import time
import os
import argparse
import requests
import numpy as np
import onnxruntime as ort
from transformers import AutoTokenizer
from datetime import datetime
import networkx as nx

class QATestSystem:
    """4ê°€ì§€ ë°©ì‹ìœ¼ë¡œ ì§ˆì˜ì‘ë‹µì„ í…ŒìŠ¤íŠ¸í•˜ëŠ” ì‹œìŠ¤í…œ"""
    
    def __init__(self, server_url, model_name="gemma3:27b",
                 onnx_model_path="model2.onnx", db_path="rag_db.json", 
                 graph_path="knowledge_graph.json", text_path="context.txt"):
        
        self.server_url = server_url
        self.model_name = model_name
        self.onnx_model_path = onnx_model_path
        self.db_path = db_path
        self.graph_path = graph_path
        self.text_path = text_path
        
        print(f"ğŸš€ QA í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        print(f"   LLM ì„œë²„: {server_url}")
        print(f"   LLM ëª¨ë¸: {model_name}")
        
        # ONNX ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹œë„
        try:
            print(f"   ONNX ëª¨ë¸ ë¡œë“œ ì¤‘: {onnx_model_path}")
            self.ort_session = ort.InferenceSession(onnx_model_path)
            self.tokenizer = AutoTokenizer.from_pretrained("intfloat/multilingual-e5-small")
            print("   ONNX ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            self.embedding_available = True
        except Exception as e:
            print(f"âš ï¸ ONNX ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("   RAG ë° GraphRAG ë°©ì‹ì€ ì„ë² ë”© ì—†ì´ ì§„í–‰ë©ë‹ˆë‹¤.")
            self.embedding_available = False
            self.ort_session = None
            self.tokenizer = None
        
        # í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ ì‹œë„
        try:
            with open(text_path, 'r', encoding='utf-8') as f:
                self.context_text = f.read()
            print(f"   ì»¨í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {text_path} ({len(self.context_text)} ì)")
            self.context_available = True
        except Exception as e:
            print(f"âš ï¸ ì»¨í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("   ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë°©ì‹ì€ ê±´ë„ˆë›°ê²Œ ë©ë‹ˆë‹¤.")
            self.context_available = False
            self.context_text = ""
        
        # DB íŒŒì¼ ë¡œë“œ ì‹œë„ (RAGìš©)
        try:
            if os.path.exists(db_path):
                print(f"   RAG DB ë¡œë“œ ì¤‘: {db_path}")
                with open(db_path, 'r', encoding='utf-8') as f:
                    self.db_data = json.load(f)
                print(f"   RAG DB ë¡œë“œ ì™„ë£Œ ({len(self.db_data['chunks'])} ì²­í¬)")
                self.rag_available = True
            else:
                print(f"âš ï¸ RAG DB íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {db_path}")
                self.rag_available = False
                self.db_data = None
        except Exception as e:
            print(f"âš ï¸ RAG DB ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.rag_available = False
            self.db_data = None
        
        # ì§€ì‹ ê·¸ë˜í”„ ë¡œë“œ ì‹œë„ (GraphRAGìš©)
        try:
            if os.path.exists(graph_path):
                print(f"   ì§€ì‹ ê·¸ë˜í”„ ë¡œë“œ ì¤‘: {graph_path}")
                self.load_graph(graph_path)
                print(f"   ì§€ì‹ ê·¸ë˜í”„ ë¡œë“œ ì™„ë£Œ ({len(self.G.nodes)} ë…¸ë“œ, {len(self.G.edges)} ì—£ì§€)")
                self.graph_available = True
            else:
                print(f"âš ï¸ ì§€ì‹ ê·¸ë˜í”„ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {graph_path}")
                self.graph_available = False
        except Exception as e:
            print(f"âš ï¸ ì§€ì‹ ê·¸ë˜í”„ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.graph_available = False
            self.G = None
            self.node_info = None
            self.node_embeddings = None
        
        print("âœ… ì´ˆê¸°í™” ì™„ë£Œ")
    
    def load_graph(self, graph_path):
        """ì§€ì‹ ê·¸ë˜í”„ ë¡œë“œ"""
        with open(graph_path, 'r', encoding='utf-8') as f:
            graph_data = json.load(f)
        
        # ê·¸ë˜í”„ ìƒì„±
        self.G = nx.DiGraph()
        
        # ë…¸ë“œ ì •ë³´ ë° ì„ë² ë”© ì €ì¥ìš© ë”•ì…”ë„ˆë¦¬
        self.node_info = {}
        self.node_embeddings = {}
        
        # ë…¸ë“œ ì¶”ê°€
        for node in graph_data.get("nodes", []):
            node_id = node.get("id")
            self.G.add_node(node_id)
            
            # ë…¸ë“œ ì •ë³´ ì €ì¥
            self.node_info[node_id] = {
                "name": node.get("name", ""),
                "type": node.get("type", ""),
                "description": node.get("description", ""),
                "properties": node.get("properties", {})
            }
            
            # ì„ë² ë”©ì´ ìˆìœ¼ë©´ ì €ì¥
            if "embedding" in node:
                self.node_embeddings[node_id] = np.array(node["embedding"])
        
        # ì—£ì§€ ì¶”ê°€
        for edge in graph_data.get("edges", []):
            source = edge.get("source")
            target = edge.get("target")
            relation = edge.get("relation", "ê´€ë ¨")
            
            if source in self.G and target in self.G:
                self.G.add_edge(source, target, relation=relation)
    
    def call_llm(self, prompt):
        """LLM API í˜¸ì¶œí•˜ì—¬ ì‘ë‹µ ë°›ê¸°"""
        try:
            response = requests.post(f"{self.server_url}/api/generate", json={
                "model": self.model_name,
                "prompt": prompt,
                "stream": False,
                "temperature": 0.1,
                "max_tokens": 2048
            })
            
            if response.status_code == 200:
                return response.json().get("response", "")
            else:
                return f"API í˜¸ì¶œ ì˜¤ë¥˜: ìƒíƒœ ì½”ë“œ {response.status_code}"
        except Exception as e:
            return f"API í˜¸ì¶œ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}"
    
    def method1_direct_query(self, question):
        """ë°©ì‹ 1: ì§ì ‘ ì§ˆë¬¸"""
        print(f"   ë°©ì‹ 1: ì§ì ‘ ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘...")
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt_start_time = time.time()
        prompt = f"ë‹¤ìŒ ì§ˆë¬¸ì— ëª…í™•í•˜ê²Œ ë‹µí•´ì£¼ì„¸ìš”:\n\nì§ˆë¬¸: {question}\n\në‹µë³€:"
        prompt_time = time.time() - prompt_start_time
        
        # ì‘ë‹µ ìƒì„±
        generation_start_time = time.time()
        response = self.call_llm(prompt)
        generation_time = time.time() - generation_start_time
        
        # ì´ ì‹œê°„
        total_time = prompt_time + generation_time
        
        return {
            "response": response,
            "prompt_time": prompt_time,
            "generation_time": generation_time,
            "total_time": total_time,
            "prompt": prompt
        }
    
    def method2_context_query(self, question):
        """ë°©ì‹ 2: ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì§ˆë¬¸"""
        print(f"   ë°©ì‹ 2: ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘...")
        
        if not self.context_available:
            return {
                "response": "ì»¨í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ì‘ë‹µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "prompt_time": 0,
                "generation_time": 0,
                "total_time": 0,
                "prompt": "ì»¨í…ìŠ¤íŠ¸ íŒŒì¼ ì—†ìŒ"
            }
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt_start_time = time.time()
        prompt = f"""ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:

[ì»¨í…ìŠ¤íŠ¸]
{self.context_text}

[ì§ˆë¬¸]
{question}

[ë‹µë³€]"""
        prompt_time = time.time() - prompt_start_time
        
        # ì‘ë‹µ ìƒì„±
        generation_start_time = time.time()
        response = self.call_llm(prompt)
        generation_time = time.time() - generation_start_time
        
        # ì´ ì‹œê°„
        total_time = prompt_time + generation_time
        
        return {
            "response": response,
            "prompt_time": prompt_time,
            "generation_time": generation_time,
            "total_time": total_time,
            "prompt": prompt
        }
    
    def get_query_embedding(self, query):
        """ì¿¼ë¦¬ í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜"""
        # E5 ëª¨ë¸ì˜ ì¿¼ë¦¬ í˜•ì‹ ì ìš©
        query_text = f"query: {query}"
        
        # í† í°í™”
        encoded_input = self.tokenizer(
            query_text,
            padding=True,
            truncation=True,
            max_length=128,
            return_tensors='np'
        )
        
        # token_type_ids ì¶”ê°€ (ONNX ëª¨ë¸ í•„ìš”)
        if 'token_type_ids' not in encoded_input:
            batch_size = encoded_input['input_ids'].shape[0]
            seq_length = encoded_input['input_ids'].shape[1]
            encoded_input['token_type_ids'] = np.zeros((batch_size, seq_length), dtype=np.int64)
        
        # ONNX ëª¨ë¸ë¡œ ì¶”ë¡ 
        model_inputs = {
            'input_ids': encoded_input['input_ids'],
            'attention_mask': encoded_input['attention_mask'],
            'token_type_ids': encoded_input['token_type_ids']
        }
        
        # ëª¨ë¸ ì‹¤í–‰
        outputs = self.ort_session.run(None, model_inputs)
        
        # ì–´í…ì…˜ ë§ˆìŠ¤í¬ë¥¼ ì‚¬ìš©í•œ í‰ê·  í’€ë§ (E5 í‘œì¤€ ë°©ì‹)
        attention_mask = encoded_input['attention_mask']
        embedding_output = outputs[0]
        
        # ë§ˆìŠ¤í¬ëœ í‰ê·  ê³„ì‚° (ë°°ì¹˜ í¬ê¸° 1)
        attention_mask = attention_mask.reshape(attention_mask.shape[0], attention_mask.shape[1], 1)
        sum_embeddings = np.sum(embedding_output * attention_mask, axis=1)
        sum_mask = np.sum(attention_mask, axis=1)
        pooled_embedding = sum_embeddings / sum_mask
        
        # ë°°ì¹˜ ì°¨ì› ì œê±° (ë°°ì¹˜ í¬ê¸° 1)
        embedding = pooled_embedding.squeeze(0)
        
        # ì •ê·œí™”
        norm = np.linalg.norm(embedding)
        normalized_embedding = embedding / norm
        
        return normalized_embedding
    
    def cosine_similarity(self, a, b):
        """ë‘ ë²¡í„° ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
    
    def search_similar(self, query, k=3):
        """ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸°"""
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = self.get_query_embedding(query)
        
        # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        db_embeddings = np.array(self.db_data["embeddings"])
        chunks = self.db_data["chunks"]
        metadatas = self.db_data["metadatas"]
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = []
        for i, doc_embedding in enumerate(db_embeddings):
            similarity = self.cosine_similarity(query_embedding, doc_embedding)
            similarities.append((i, similarity))
        
        # ìœ ì‚¬ë„ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        # ìƒìœ„ kê°œ ê²°ê³¼ ë°˜í™˜
        results = []
        for idx, sim in similarities[:k]:
            results.append({
                "text": chunks[idx],
                "metadata": metadatas[idx],
                "similarity": sim
            })
        
        return results
    
    def format_rag_prompt(self, contexts, query):
        """ê²€ìƒ‰ ê²°ê³¼ì™€ ì§ˆë¬¸ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        context_text = "\n\n".join([f"[ì¶œì²˜: {ctx['metadata']['source']}]\n{ctx['text']}" 
                                  for ctx in contexts])
        
        return f"""ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:

[ì°¸ê³  ì •ë³´]
{context_text}

[ì§ˆë¬¸]
{query}

[ë‹µë³€]"""
    
    def method3_rag_query(self, question, k=3):
        """ë°©ì‹ 3: RAG ê¸°ë°˜ ì§ˆë¬¸"""
        print(f"   ë°©ì‹ 3: RAG ê¸°ë°˜ ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘...")
        
        if not self.rag_available or not self.embedding_available:
            return {
                "response": "RAG ë°ì´í„°ë² ì´ìŠ¤ ë˜ëŠ” ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ì‘ë‹µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "prompt_time": 0,
                "generation_time": 0,
                "total_time": 0,
                "prompt": "RAG ë¶ˆê°€ëŠ¥",
                "contexts": []
            }
        
        prompt_start_time = time.time()
        
        # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        contexts = self.search_similar(question, k)
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self.format_rag_prompt(contexts, question)
        prompt_time = time.time() - prompt_start_time
        
        # ì‘ë‹µ ìƒì„±
        generation_start_time = time.time()
        response = self.call_llm(prompt)
        generation_time = time.time() - generation_start_time
        
        # ì´ ì‹œê°„
        total_time = prompt_time + generation_time
        
        return {
            "response": response,
            "prompt_time": prompt_time,
            "generation_time": generation_time,
            "total_time": total_time,
            "prompt": prompt,
            "contexts": contexts
        }
    
    def search_graph(self, query, top_k=10):
        """ì§€ì‹ ê·¸ë˜í”„ì—ì„œ ê²€ìƒ‰"""
        print(f"ğŸ” ê·¸ë˜í”„ ê²€ìƒ‰ ì‹œì‘: '{query}' (top_k={top_k})")
        
        start_time = time.time()
        results = []
        
        # ì¿¼ë¦¬ ì„ë² ë”© ê³„ì‚°
        query_embedding = None
        if self.embedding_available and self.ort_session is not None:
            try:
                query_embedding = self.get_query_embedding(query)
            except Exception as e:
                print(f"âš ï¸ ì¿¼ë¦¬ ì„ë² ë”© ê³„ì‚° ì‹¤íŒ¨: {e}")
        
        # ì„ë² ë”© ìœ ì‚¬ë„ ê²€ìƒ‰
        embedding_results = []
        if query_embedding is not None:
            print("   ì„ë² ë”© ê¸°ë°˜ ê²€ìƒ‰ ìˆ˜í–‰ ì¤‘...")
            
            # ëª¨ë“  ë…¸ë“œì™€ì˜ ìœ ì‚¬ë„ ê³„ì‚°
            for node_id, node_info in self.node_info.items():
                node_embedding = self.get_node_embedding(node_id)
                if node_embedding is not None:
                    sim = self.cosine_similarity(query_embedding, node_embedding)
                    embedding_results.append((node_id, sim, "ì„ë² ë”© ìœ ì‚¬ë„"))
            
            # ìƒìœ„ ê²°ê³¼ ì„ íƒ
            embedding_results.sort(key=lambda x: x[1], reverse=True)
            embedding_results = embedding_results[:top_k*2]  # ì¶©ë¶„íˆ ë§ì€ í›„ë³´ ì„ íƒ
            print(f"   ì„ë² ë”© ê²€ìƒ‰ ê²°ê³¼: {len(embedding_results)}ê°œ ë…¸ë“œ ë°œê²¬")
        
        # í…ìŠ¤íŠ¸ ë§¤ì¹­ ê²€ìƒ‰
        keyword_results = []
        if len(embedding_results) < top_k:
            print("   í…ìŠ¤íŠ¸ ë§¤ì¹­ ê²€ìƒ‰ ìˆ˜í–‰ ì¤‘...")
            
            query_tokens = set(query.lower().split())
            for node_id, node_info in self.node_info.items():
                # ë…¸ë“œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                node_text = " ".join([
                    node_info.get("name", ""),
                    node_info.get("description", ""),
                    str(node_info.get("properties", {}))
                ]).lower()
                
                # í† í° ë§¤ì¹­ ì ìˆ˜
                matches = sum(1 for token in query_tokens if token in node_text)
                if matches > 0:
                    score = matches / len(query_tokens)
                    keyword_results.append((node_id, score, "í…ìŠ¤íŠ¸ ë§¤ì¹­"))
            
            # ìƒìœ„ ê²°ê³¼ ì„ íƒ
            keyword_results.sort(key=lambda x: x[1], reverse=True)
            keyword_results = keyword_results[:top_k]
            print(f"   í…ìŠ¤íŠ¸ ë§¤ì¹­ ê²°ê³¼: {len(keyword_results)}ê°œ ë…¸ë“œ ë°œê²¬")
        
        # ê²°ê³¼ í•©ì¹˜ê¸°
        all_results = embedding_results + keyword_results
        all_results.sort(key=lambda x: x[1], reverse=True)
        
        # ì¤‘ë³µ ì œê±° (ë†’ì€ ì ìˆ˜ ìœ ì§€)
        seen_nodes = set()
        filtered_results = []
        for node_id, score, method in all_results:
            if node_id not in seen_nodes:
                seen_nodes.add(node_id)
                filtered_results.append((node_id, score, method))
        
        # ê·¸ë˜í”„ ê¸°ë°˜ í™•ì¥ (ê´€ë ¨ ë…¸ë“œ í¬í•¨)
        final_results = list(filtered_results)
        if len(filtered_results) > 0:
            print("   ê·¸ë˜í”„ ê¸°ë°˜ í™•ì¥ ìˆ˜í–‰ ì¤‘...")
            expansion_nodes = set()
            
            # ìµœìƒìœ„ ë…¸ë“œ ì£¼ë³€ ê·¸ë˜í”„ íƒìƒ‰
            for node_id, _, _ in filtered_results[:min(3, len(filtered_results))]:
                # ë“¤ì–´ì˜¤ëŠ” ì—£ì§€
                for edge in self.G.predecessors(node_id):
                    pred = edge
                    if pred and pred not in seen_nodes:
                        expansion_nodes.add((pred, 0.7, "ê·¸ë˜í”„ ì—°ê²°"))
                
                # ë‚˜ê°€ëŠ” ì—£ì§€
                for edge in self.G.successors(node_id):
                    succ = edge
                    if succ and succ not in seen_nodes:
                        expansion_nodes.add((succ, 0.6, "ê·¸ë˜í”„ ì—°ê²°"))
            
            # í™•ì¥ëœ ë…¸ë“œ ì¶”ê°€
            for node_id, score, method in expansion_nodes:
                final_results.append((node_id, score, method))
                seen_nodes.add(node_id)
        
        # ìµœì¢… ê²°ê³¼ ì •ë ¬ ë° ìƒìœ„ ì„ íƒ
        final_results.sort(key=lambda x: x[1], reverse=True)
        final_results = final_results[:top_k]
        
        # ê²°ê³¼ ì •ë³´ êµ¬ì„±
        for node_id, score, method in final_results:
            node_info = self.node_info.get(node_id, {})
            
            # ì—£ì§€ ì •ë³´ ìˆ˜ì§‘
            incoming_edges = []
            for pred in self.G.predecessors(node_id):
                edge_data = self.G.get_edge_data(pred, node_id)
                pred_info = self.node_info.get(pred, {})
                incoming_edges.append({
                    "source_id": pred,
                    "source_name": pred_info.get("name", "ì•Œ ìˆ˜ ì—†ìŒ"),
                    "relation": edge_data.get("relation", "ê´€ë ¨")
                })
            
            outgoing_edges = []
            for succ in self.G.successors(node_id):
                edge_data = self.G.get_edge_data(node_id, succ)
                succ_info = self.node_info.get(succ, {})
                outgoing_edges.append({
                    "target_id": succ,
                    "target_name": succ_info.get("name", "ì•Œ ìˆ˜ ì—†ìŒ"),
                    "relation": edge_data.get("relation", "ê´€ë ¨")
                })
            
            # ê²°ê³¼ ì¶”ê°€
            results.append({
                "id": node_id,
                "name": node_info.get("name", ""),
                "type": node_info.get("type", ""),
                "description": node_info.get("description", ""),
                "properties": node_info.get("properties", {}),
                "score": score,
                "method": method,
                "incoming_edges": incoming_edges,
                "outgoing_edges": outgoing_edges
            })
        
        search_time = time.time() - start_time
        print(f"âœ… ê·¸ë˜í”„ ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ë…¸ë“œ ({search_time:.2f}ì´ˆ)")
        
        return results
    
    def get_node_embedding(self, node_id):
        """ë…¸ë“œ ì„ë² ë”© ê°€ì ¸ì˜¤ê¸° (ìºì‹œ ë˜ëŠ” ê³„ì‚°)"""
        if not self.embedding_available or self.ort_session is None:
            return None
        
        # ìºì‹œì— ìˆìœ¼ë©´ ì¬ì‚¬ìš©
        if node_id in self.node_embeddings:
            return self.node_embeddings[node_id]
        
        # ë…¸ë“œ í…ìŠ¤íŠ¸ ì¤€ë¹„
        node_info = self.node_info.get(node_id, {})
        node_text = " ".join([
            node_info.get("name", ""),
            node_info.get("description", ""),
            str(node_info.get("properties", {}))
        ])
        
        # ì„ë² ë”© ê³„ì‚°
        try:
            embedding = self.compute_embedding(node_text)
            if embedding is not None:
                self.node_embeddings[node_id] = embedding
            return embedding
        except Exception as e:
            print(f"âš ï¸ ë…¸ë“œ ì„ë² ë”© ê³„ì‚° ì‹¤íŒ¨({node_id}): {e}")
            return None
    
    def compute_embedding(self, text):
        """í…ìŠ¤íŠ¸ì— ëŒ€í•œ ì„ë² ë”© ê³„ì‚°"""
        if not self.embedding_available or self.ort_session is None:
            return None
        
        try:
            # í† í¬ë‚˜ì´ì§•
            inputs = self.tokenizer(text, padding=True, truncation=True, 
                                   return_tensors="np", max_length=512)
            
            # ONNX ëª¨ë¸ ì‹¤í–‰ì„ ìœ„í•œ ì…ë ¥ ì¤€ë¹„
            ort_inputs = {
                "input_ids": inputs["input_ids"].astype(np.int64),
                "attention_mask": inputs["attention_mask"].astype(np.int64)
            }
            
            # token_type_idsê°€ í•„ìš”í•˜ì§€ë§Œ í† í¬ë‚˜ì´ì €ì—ì„œ ì œê³µí•˜ì§€ ì•ŠëŠ” ê²½ìš° ì§ì ‘ ìƒì„±
            if "token_type_ids" not in inputs and "token_type_ids" in [input.name for input in self.ort_session.get_inputs()]:
                token_type_ids = np.zeros_like(inputs["input_ids"])
                ort_inputs["token_type_ids"] = token_type_ids
            # í† í¬ë‚˜ì´ì €ì—ì„œ ì œê³µí•˜ëŠ” ê²½ìš°
            elif "token_type_ids" in inputs:
                ort_inputs["token_type_ids"] = inputs["token_type_ids"].astype(np.int64)
            
            # ONNX ì‹¤í–‰
            embeddings = self.ort_session.run(None, ort_inputs)[0]
            
            # ë§ˆìŠ¤í‚¹ëœ í† í°ì˜ í‰ê·  ì„ë² ë”© ê³„ì‚° (CLS í† í° ë˜ëŠ” í‰ê· )
            return embeddings[0, 0]  # CLS í† í° ì‚¬ìš©
        except Exception as e:
            print(f"âš ï¸ ì„ë² ë”© ê³„ì‚° ì‹¤íŒ¨: {e}")
            return None
    
    def build_graph_prompt(self, query, search_results):
        """ê²€ìƒ‰ ê²°ê³¼ë¡œë¶€í„° ê·¸ë˜í”„ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        prompt = f"""### ì§ˆë¬¸:
{query}

### ê´€ë ¨ ì •ë³´:
"""
        
        # ë…¸ë“œ ìœ í˜•ë³„ ê·¸ë£¹í™”
        node_groups = {}
        for result in search_results:
            node_type = result.get("type", "ê¸°íƒ€")
            if node_type not in node_groups:
                node_groups[node_type] = []
            node_groups[node_type].append(result)
        
        # ìœ í˜•ë³„ë¡œ ë…¸ë“œ ì •ë³´ ì¶”ê°€
        for node_type, nodes in node_groups.items():
            prompt += f"\n## {node_type} ì •ë³´:\n"
            
            for node in nodes:
                prompt += f"\n### {node.get('name', 'ì´ë¦„ ì—†ìŒ')}\n"
                
                # ì„¤ëª… ì¶”ê°€
                if node.get("description"):
                    prompt += f"{node['description']}\n"
                
                # ì†ì„± ì¶”ê°€
                if node.get("properties"):
                    prompt += "\nì†ì„±:\n"
                    for key, value in node["properties"].items():
                        prompt += f"- {key}: {value}\n"
                
                # ê´€ê³„ ì •ë³´ ì¶”ê°€
                if node.get("incoming_edges"):
                    prompt += "\në“¤ì–´ì˜¤ëŠ” ê´€ê³„:\n"
                    for edge in node["incoming_edges"]:
                        prompt += f"- {edge['source_name']} â†’ {edge['relation']} â†’ {node['name']}\n"
                
                if node.get("outgoing_edges"):
                    prompt += "\në‚˜ê°€ëŠ” ê´€ê³„:\n"
                    for edge in node["outgoing_edges"]:
                        prompt += f"- {node['name']} â†’ {edge['relation']} â†’ {edge['target_name']}\n"
        
        # LLM ì§€ì‹œ ì¶”ê°€
        prompt += f"""
### ì§€ì‹œì‚¬í•­:
- ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
- ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ ì†”ì§íˆ ëª¨ë¥¸ë‹¤ê³  ë§í•´ì£¼ì„¸ìš”.
- ë‹µë³€ì€ ëª…í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
- ê°€ëŠ¥í•˜ë©´ ì œê³µëœ ì •ë³´ì˜ ì¶œì²˜ë¥¼ ì–¸ê¸‰í•´ì£¼ì„¸ìš”.

### ë‹µë³€:
"""
        
        return prompt
    
    def method4_graph_rag_query(self, question, top_k=10):
        """ë°©ì‹ 4: GraphRAG ê¸°ë°˜ ì§ˆë¬¸"""
        print(f"   ë°©ì‹ 4: GraphRAG ê¸°ë°˜ ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘...")
        
        if not self.graph_available:
            return {
                "response": "ì§€ì‹ ê·¸ë˜í”„ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ì‘ë‹µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "prompt_time": 0,
                "generation_time": 0,
                "total_time": 0,
                "prompt": "GraphRAG ë¶ˆê°€ëŠ¥",
                "search_results": []
            }
        
        prompt_start_time = time.time()
        
        # ê·¸ë˜í”„ì—ì„œ ê´€ë ¨ ë…¸ë“œ ê²€ìƒ‰
        search_results = self.search_graph(question, top_k)
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = self.build_graph_prompt(question, search_results)
        prompt_time = time.time() - prompt_start_time
        
        # ì‘ë‹µ ìƒì„±
        generation_start_time = time.time()
        response = self.call_llm(prompt)
        generation_time = time.time() - generation_start_time
        
        # ì´ ì‹œê°„
        total_time = prompt_time + generation_time
        
        return {
            "response": response,
            "prompt_time": prompt_time,
            "generation_time": generation_time,
            "total_time": total_time,
            "prompt": prompt,
            "search_results": search_results
        }
    
    def process_question(self, question):
        """4ê°€ì§€ ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸ ì²˜ë¦¬"""
        print(f"\nğŸ“ ì§ˆë¬¸ ì²˜ë¦¬: '{question}'")
        
        results = {
            "question": question,
            "timestamp": datetime.now().isoformat(),
            "methods": {}
        }
        
        # ë°©ì‹ 1: ì§ì ‘ ì§ˆë¬¸
        method1_result = self.method1_direct_query(question)
        results["methods"]["direct_query"] = method1_result
        print(f"   ë°©ì‹ 1 ì™„ë£Œ: {method1_result['total_time']:.2f}ì´ˆ")
        
        # ë°©ì‹ 2: ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì§ˆë¬¸
        method2_result = self.method2_context_query(question)
        results["methods"]["context_query"] = method2_result
        print(f"   ë°©ì‹ 2 ì™„ë£Œ: {method2_result['total_time']:.2f}ì´ˆ")
        
        # ë°©ì‹ 3: RAG ê¸°ë°˜ ì§ˆë¬¸
        method3_result = self.method3_rag_query(question)
        results["methods"]["rag_query"] = method3_result
        print(f"   ë°©ì‹ 3 ì™„ë£Œ: {method3_result['total_time']:.2f}ì´ˆ")
        
        # ë°©ì‹ 4: GraphRAG ê¸°ë°˜ ì§ˆë¬¸
        method4_result = self.method4_graph_rag_query(question)
        results["methods"]["graph_rag_query"] = method4_result
        print(f"   ë°©ì‹ 4 ì™„ë£Œ: {method4_result['total_time']:.2f}ì´ˆ")
        
        return results

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ëª…ë ¹ì¤„ ì¸í„°í˜ì´ìŠ¤ ì„¤ì •
    parser = argparse.ArgumentParser(description="QA ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ë„êµ¬")
    parser.add_argument("--qa_file", default="qa_dataset_1_300.json", help="QA ë°ì´í„°ì…‹ íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--output", default="qa_test_results.json", help="ê²°ê³¼ ì €ì¥ íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--server", help="LLM ì„œë²„ URL")
    parser.add_argument("--model", default="gemma3:27b", help="LLM ëª¨ë¸ ì´ë¦„")
    parser.add_argument("--onnx_model", default="model2.onnx", help="ONNX ëª¨ë¸ íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--db", default="rag_db.json", help="RAG ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ")
    parser.add_argument("--graph", default="knowledge_graph.json", help="ì§€ì‹ ê·¸ë˜í”„ íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--text", default="context.txt", help="ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--limit", type=int, default=10, help="ì²˜ë¦¬í•  ì§ˆë¬¸ ìˆ˜ ì œí•œ (0ì€ ëª¨ë‘ ì²˜ë¦¬)")
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸš€ QA í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ")
    print("=" * 60)
    
    # QA ë°ì´í„°ì…‹ ë¡œë“œ
    try:
        with open(args.qa_file, 'r', encoding='utf-8') as f:
            qa_data = json.load(f)
        print(f"âœ… QA ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {args.qa_file} ({len(qa_data)} ê°œ ì§ˆë¬¸)")
    except Exception as e:
        print(f"âŒ QA ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    # í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    qa_system = QATestSystem(
        server_url=args.server,
        model_name=args.model,
        onnx_model_path=args.onnx_model,
        db_path=args.db,
        graph_path=args.graph,
        text_path=args.text
    )
    
    # ì§ˆë¬¸ ì²˜ë¦¬
    all_results = []
    question_count = min(args.limit, len(qa_data)) if args.limit > 0 else len(qa_data)
    
    print(f"\nğŸ“Š {question_count}ê°œ ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘...")
    
    for i, qa_item in enumerate(qa_data[:question_count], 1):
        question = qa_item["question"]
        print(f"\n[{i}/{question_count}] ì§ˆë¬¸: '{question}'")
        
        # ì§ˆë¬¸ ì²˜ë¦¬
        result = qa_system.process_question(question)
        all_results.append(result)
        
        # ì§„í–‰ ìƒí™© í‘œì‹œ
        print(f"âœ… ì§ˆë¬¸ {i}/{question_count} ì™„ë£Œ")
        
        # ê²°ê³¼ ì €ì¥ (ì¤‘ê°„ ì €ì¥)
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)
        
        # ë§ˆì§€ë§‰ ì§ˆë¬¸ì´ ì•„ë‹ˆë©´ ì ì‹œ ëŒ€ê¸°
        if i < question_count:
            time.sleep(1)
    
    # ìµœì¢… ê²°ê³¼ ìš”ì•½
    print("\nğŸ“‹ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½:")
    
    # ê° ë°©ì‹ë³„ í‰ê·  ì‹œê°„ ê³„ì‚°
    method_times = {
        "direct_query": {"prompt": 0, "generation": 0, "total": 0},
        "context_query": {"prompt": 0, "generation": 0, "total": 0},
        "rag_query": {"prompt": 0, "generation": 0, "total": 0},
        "graph_rag_query": {"prompt": 0, "generation": 0, "total": 0}
    }
    
    for result in all_results:
        for method_name, method_data in result["methods"].items():
            method_times[method_name]["prompt"] += method_data["prompt_time"]
            method_times[method_name]["generation"] += method_data["generation_time"]
            method_times[method_name]["total"] += method_data["total_time"]
    
    for method_name, times in method_times.items():
        avg_prompt = times["prompt"] / len(all_results)
        avg_generation = times["generation"] / len(all_results)
        avg_total = times["total"] / len(all_results)
        
        print(f"\në°©ì‹: {method_name}")
        print(f"  - í‰ê·  í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹œê°„: {avg_prompt:.4f}ì´ˆ")
        print(f"  - í‰ê·  ì‘ë‹µ ìƒì„± ì‹œê°„: {avg_generation:.4f}ì´ˆ")
        print(f"  - í‰ê·  ì´ ì‹œê°„: {avg_total:.4f}ì´ˆ")
    
    print(f"\nğŸ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ. ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {args.output}")

if __name__ == "__main__":
    main() 